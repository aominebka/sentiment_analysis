{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"processingdata.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xqqX5RLuAylm","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import csv\n","import re\n","import random\n","import numpy as np\n","\n","from IPython import embed\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4DauJxN1liI","colab_type":"code","outputId":"a274e94f-45a6-41d8-e6a9-8b59eaa46db7","executionInfo":{"status":"ok","timestamp":1571058320775,"user_tz":-420,"elapsed":13427,"user":{"displayName":"hi trum","photoUrl":"","userId":"18068657964817083524"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["from google_drive_downloader import GoogleDriveDownloader as gdd\n","gdd.download_file_from_google_drive(file_id='1FNl79LtBEnhXx4NKoPubwThERuZC6WtP', dest_path='./Sentiment Analysis Dataset.zip', unzip=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading 1FNl79LtBEnhXx4NKoPubwThERuZC6WtP into ./Sentiment Analysis Dataset.zip... Done.\n","Unzipping...Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QHKki7xeWbA6","colab_type":"code","outputId":"e213c1b1-17cc-469a-d012-58051fc3f9ed","executionInfo":{"status":"ok","timestamp":1571058322134,"user_tz":-420,"elapsed":14779,"user":{"displayName":"hi trum","photoUrl":"","userId":"18068657964817083524"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["\n","gdd.download_file_from_google_drive(file_id='1ZXLowlLD161FlvLwcBjZ4TqAHxmiiket', dest_path='./processda1.zip', unzip=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading 1ZXLowlLD161FlvLwcBjZ4TqAHxmiiket into ./processda1.zip... Done.\n","Unzipping...Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mO7Q6ESgA4jR","colab_type":"code","colab":{}},"source":["def separate_dataset(filename):\n","    good_out = open(\"good_\"+filename,\"w+\");\n","    bad_out  = open(\"bad_\"+filename,\"w+\");\n","\n","\n","    with open(filename,'r') as f:\n","        reader = csv.reader(f)\n","        \n","\n","        for line in reader:\n","           \n","            sentiment = line[1]\n","            sentence = line[3]\n","\n","            if sentiment == \"0\":\n","                bad_out.write(sentence+\"\\n\")\n","            elif sentiment == \"1\":\n","                good_out.write(sentence+\"\\n\")\n","\n","\n","    good_out.close();\n","    bad_out.close();\n","\n","separate_dataset(\"Sentiment Analysis Dataset.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tUJfJ6RW4gZu","colab_type":"text"},"source":["# Pre processing data \n","\n"]},{"cell_type":"code","metadata":{"id":"JpAbQ-EnSPbk","colab_type":"code","outputId":"4c567287-5e03-48b5-bc2d-7dc28eccb7da","executionInfo":{"status":"ok","timestamp":1571058343880,"user_tz":-420,"elapsed":36513,"user":{"displayName":"hi trum","photoUrl":"","userId":"18068657964817083524"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!apt install -qq enchant\n","!pip install pyenchant\n","import re\n","import enchant\n","import itertools\n","\n","\n","\n","\n","dico = {}\n","dico1 = open('dicos/dico1.txt', 'rb')\n","for word in dico1:\n","    word = word.decode('utf8')\n","    word = word.split()\n","    dico[word[1]] = word[3]\n","dico1.close()\n","dico2 = open('dicos/dico2.txt', 'rb')\n","for word in dico2:\n","    word = word.decode('utf8')\n","    word = word.split()\n","    dico[word[0]] = word[1]\n","dico2.close()\n","dico3 = open('dicos/dico3.txt', 'rb')\n","for word in dico3:\n","    word = word.decode('utf8')\n","    word = word.split()\n","    dico[word[0]] = word[1]\n","dico3.close()\n","\n","d = enchant.Dict('en_US')\n","\n","def remove_repetitions(tweet):\n","    \n","    tweet=tweet.split()\n","    \n","    for i in range(len(tweet)):\n","        if tweet[i]=='<allcaps>': continue\n","        tweet[i]=''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet[i])).replace('#', '')\n","        if len(tweet[i])>0:\n","            if not d.check(tweet[i]):\n","                tweet[i] = ''.join(''.join(s)[:1] for _, s in itertools.groupby(tweet[i])).replace('#', '')\n","    tweet=' '.join(tweet)\n","    return tweet\n","\n","def correct_spell(tweet):\n","\n","    tweet = tweet.split()\n","    for i in range(len(tweet)):\n","        if tweet[i] in dico.keys():\n","            tweet[i] = dico[tweet[i]]\n","    tweet = ' '.join(tweet)\n","    return tweet\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The following additional packages will be installed:\n","  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n","  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n","Suggested packages:\n","  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n","  | openoffice.org-core libenchant-voikko\n","The following NEW packages will be installed:\n","  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n","  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n","0 upgraded, 10 newly installed, 0 to remove and 8 not upgraded.\n","Need to get 1,300 kB of archives.\n","After this operation, 5,353 kB of additional disk space will be used.\n","Preconfiguring packages ...\n","Selecting previously unselected package libtext-iconv-perl.\n","(Reading database ... 131183 files and directories currently installed.)\n","Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n","Unpacking libtext-iconv-perl (1.7-5build6) ...\n","Selecting previously unselected package libaspell15:amd64.\n","Preparing to unpack .../1-libaspell15_0.60.7~20110707-4_amd64.deb ...\n","Unpacking libaspell15:amd64 (0.60.7~20110707-4) ...\n","Selecting previously unselected package emacsen-common.\n","Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n","Unpacking emacsen-common (2.0.8) ...\n","Selecting previously unselected package dictionaries-common.\n","Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n","Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n","Unpacking dictionaries-common (1.27.2) ...\n","Selecting previously unselected package aspell.\n","Preparing to unpack .../4-aspell_0.60.7~20110707-4_amd64.deb ...\n","Unpacking aspell (0.60.7~20110707-4) ...\n","Selecting previously unselected package aspell-en.\n","Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n","Unpacking aspell-en (2017.08.24-0-0.1) ...\n","Selecting previously unselected package hunspell-en-us.\n","Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n","Unpacking hunspell-en-us (1:2017.08.24) ...\n","Selecting previously unselected package libhunspell-1.6-0:amd64.\n","Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n","Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n","Selecting previously unselected package libenchant1c2a:amd64.\n","Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n","Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n","Selecting previously unselected package enchant.\n","Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n","Unpacking enchant (1.6.0-11.1) ...\n","Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n","Setting up libaspell15:amd64 (0.60.7~20110707-4) ...\n","Setting up emacsen-common (2.0.8) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Setting up libtext-iconv-perl (1.7-5build6) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Setting up dictionaries-common (1.27.2) ...\n","Setting up aspell (0.60.7~20110707-4) ...\n","Setting up hunspell-en-us (1:2017.08.24) ...\n","Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n","Setting up aspell-en (2017.08.24-0-0.1) ...\n","Setting up enchant (1.6.0-11.1) ...\n","Processing triggers for dictionaries-common (1.27.2) ...\n","aspell-autobuildhash: processing: en [en-common].\n","aspell-autobuildhash: processing: en [en-variant_0].\n","aspell-autobuildhash: processing: en [en-variant_1].\n","aspell-autobuildhash: processing: en [en-variant_2].\n","aspell-autobuildhash: processing: en [en-w_accents-only].\n","aspell-autobuildhash: processing: en [en-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_AU-variant_0].\n","aspell-autobuildhash: processing: en [en_AU-variant_1].\n","aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n","aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_CA-variant_0].\n","aspell-autobuildhash: processing: en [en_CA-variant_1].\n","aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n","aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-variant_0].\n","aspell-autobuildhash: processing: en [en_GB-variant_1].\n","aspell-autobuildhash: processing: en [en_US-w_accents-only].\n","aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Collecting pyenchant\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/54/04d88a59efa33fefb88133ceb638cdf754319030c28aadc5a379d82140ed/pyenchant-2.0.0.tar.gz (64kB)\n","\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyenchant\n","  Building wheel for pyenchant (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyenchant: filename=pyenchant-2.0.0-cp36-none-any.whl size=71113 sha256=0fca1bf9ae5570059118f1315346fb9e6acd9c960b25ddd4b06aa9e038ee4320\n","  Stored in directory: /root/.cache/pip/wheels/ee/8e/01/f427e9c6c0ae5e22095f3d2aac8997abe7b317307a9de497f4\n","Successfully built pyenchant\n","Installing collected packages: pyenchant\n","Successfully installed pyenchant-2.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tPS_OGpi2f1p","colab_type":"code","outputId":"4270cf74-5e57-4afc-ac7b-65a84f909a05","executionInfo":{"status":"ok","timestamp":1571058361539,"user_tz":-420,"elapsed":54165,"user":{"displayName":"hi trum","photoUrl":"","userId":"18068657964817083524"}},"colab":{"base_uri":"https://localhost:8080/","height":211}},"source":["!pip install regex"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting regex\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n","\u001b[K     |████████████████████████████████| 655kB 6.3MB/s \n","\u001b[?25hBuilding wheels for collected packages: regex\n","  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609241 sha256=35bb4e6b38aa35be931d3179cf9739ec9149395adf068801329393f0e58e9293\n","  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n","Successfully built regex\n","Installing collected packages: regex\n","Successfully installed regex-2019.8.19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WRcjdfaI4orh","colab_type":"code","colab":{}},"source":["import sys\n","import regex as re\n","\n","FLAGS = re.MULTILINE | re.DOTALL\n","\n","def hashtag(text):\n","    text = text.group()\n","    hashtag_body = text[1:]\n","    if hashtag_body.isupper():\n","        result = \" {} \".format(hashtag_body.lower())\n","    else:\n","        result = \" <hashtag> \" + \" \".join([\"\"] + [re.sub(r\"([A-Z])\",r\" \\1\", hashtag_body, flags=FLAGS)])+\" <allcaps> \"\n","    return result\n","\n","\n","\n","\n","def tokenize(text):\n","    # Different regex parts for smiley faces\n","    eyes = r\"[8:=;]\"\n","    nose = r\"['`\\-]?\"\n","\n","    # function so code less repetitive\n","    def re_sub(pattern, repl):\n","        return re.sub(pattern, repl, text, flags=FLAGS)\n","\n","    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" <url> \")\n","    text = re_sub(r\"@\\w+\", \"<user>\")\n","    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \" <smile> \")\n","    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n","    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \" <sadface> \")\n","    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \" <neutralface> \")\n","    text = re_sub(r\"/\",\" / \")\n","    text = re_sub(r\"<3\",\"<heart>\")\n","    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" <number> \")\n","    text = re_sub(r\"#\\S+\", hashtag)\n","    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat> \")\n","  #  text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n","\n","\n","    return text.lower()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONiChC2A12N2","colab_type":"code","colab":{}},"source":["def emoji_translation(text):\n","    loves = [\"<3\", \"♥\"]\n","    smilefaces = []\n","    sadfaces = []\n","    neutralfaces = []\n","\n","    eyes = [\"8\",\":\",\"=\",\";\"]\n","    nose = [\"'\",\"`\",\"-\",r\"\\\\\"]\n","    for e in eyes:\n","        for n in nose:\n","            for s in [\"\\)\", \"d\", \"]\", \"}\",\"p\"]:\n","                smilefaces.append(e+n+s)\n","                smilefaces.append(e+s)\n","            for s in [\"\\(\", \"\\[\", \"{\"]:\n","                sadfaces.append(e+n+s)\n","                sadfaces.append(e+s)\n","            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n","                neutralfaces.append(e+n+s)\n","                neutralfaces.append(e+s)\n","            #reversed\n","            for s in [\"\\(\", \"\\[\", \"{\"]:\n","                smilefaces.append(s+n+e)\n","                smilefaces.append(s+e)\n","            for s in [\"\\)\", \"\\]\", \"}\"]:\n","                sadfaces.append(s+n+e)\n","                sadfaces.append(s+e)\n","            for s in [\"\\|\", \"\\/\", r\"\\\\\"]:\n","                neutralfaces.append(s+n+e)\n","                neutralfaces.append(s+e)\n","\n","    smilefaces = list(set(smilefaces))\n","    sadfaces = list(set(sadfaces))\n","    neutralfaces = list(set(neutralfaces))\n","\n","    t = []\n","    for w in text.split():\n","        if w in loves:\n","            t.append(\"<heart>\")\n","        elif w in smilefaces:\n","            t.append(\"<smile>\")\n","        elif w in neutralfaces:\n","            t.append(\"<neutralface>\")\n","        elif w in sadfaces:\n","            t.append(\"<sadface>\")\n","        else:\n","            t.append(w)\n","    newText = \" \".join(t)\n","    return newText"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cYIGuUmtA4hK","colab_type":"code","colab":{}},"source":["#Clean Dataset\n","def clean_str(string):\n","  \n","    string= tokenize(string)\n","    string=emoji_translation(string)  \n","\n","    #EMOJIS\n","    string = re.sub(r\":\\)\",\" <smile> \",string)\n","    string = re.sub(r\":>\",\" <smile> \",string)\n","    string = re.sub(r\" XD\",\" <smile> \",string)\n","    string = re.sub(r\" <3\",\" <heart> \",string)\n","    string = re.sub(r\" :p\",\" <lolface> \",string)\n","    string = re.sub(r\" :P\",\" <lolface> \",string)\n","    string = re.sub(r\":\\(\",\" <sadface> \",string)\n","    string = re.sub(r\":<\",\" <sadface> \",string)\n","    string = re.sub(r\":<\",\" <sadface> \",string)\n","    string = re.sub(r\">:\\(\",\" <sadface> \",string)\n","\n","\n","    #STRANGE UNICODE \\x...\n","    string = re.sub(r\"\\\\x(\\S)*\",\"\",string)\n","\n","\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","\n","    string = re.sub(r\"i\\'m\", \"i am\", string)\n","    string = re.sub(r\"\\'re\", \"are\", string)\n","    string = re.sub(r\"he\\'s\", \"he is\", string)\n","    string = re.sub(r\"it\\'s\", \"it is\", string)\n","    string = re.sub(r\"that\\'s\", \"that is\", string)\n","    string = re.sub(r\"who\\'s\", \"who is\", string)\n","    string = re.sub(r\"what\\'s\", \"what is\", string)\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"n\\'t\", \" not\", string)\n","    string = re.sub(r\"\\'ve\", \" have\", string)\n","    string = re.sub(r\"\\'d\", \" would\", string)\n","    string = re.sub(r\"\\'ll\", \" will\", string)\n","    \n","    string = re.sub(r\",\", \" , \", string)\n","    string = re.sub(r\"!\", \" ! \", string)\n","    string = re.sub(r\"\\.\", \" . \", string)\n","    string = re.sub(r\"\\(\", \" ( \", string)\n","    string = re.sub(r\"\\)\", \" ) \", string)\n","    string = re.sub(r\"\\?\", \" ? \", string)\n","    string = remove_repetitions(string)\n","    string = correct_spell(string)   \n","                  \n","    return string.strip().lower()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6cfA6tKiMCE","colab_type":"code","outputId":"d4da17ca-040e-4def-ccf0-970d2d635e7a","executionInfo":{"status":"ok","timestamp":1571058361542,"user_tz":-420,"elapsed":54147,"user":{"displayName":"hi trum","photoUrl":"","userId":"18068657964817083524"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["s=\" #MachineLera lonnnn will :) looooooveeeee zooo #DIT #SickedLee I'm 2th    kkk\"\n","print(clean_str(s))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<hashtag> machine lera <allcaps> lon will <smile> love zoo it <hashtag> sicked lee <allcaps> i am <number> to ok\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CXemMsTCA4e6","colab_type":"code","colab":{}},"source":["def get_dataset(goodfile,badfile,limit,randomize=True):\n","    good_x = list(open(goodfile,\"r\").readlines())\n","    good_x = [s.strip() for s in good_x]\n","    \n","    bad_x  = list(open(badfile,\"r\").readlines())\n","    bad_x  = [s.strip() for s in bad_x]\n","\n","    if (randomize):\n","        random.shuffle(bad_x)\n","        random.shuffle(good_x)\n","\n","    good_x = good_x[:limit]\n","    bad_x = bad_x[:limit]\n","\n","    x = good_x + bad_x\n","    x = [clean_str(s) for s in x]\n","\n","\n","    positive_labels = [[1] for _ in good_x]\n","    negative_labels = [[0] for _ in bad_x]\n","    y = np.concatenate([positive_labels, negative_labels], 0)\n","    return [x,y]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKbu_f8sA4cq","colab_type":"code","colab":{}},"source":["def gen_batch(data, batch_size, num_epochs, shuffle=True):\n","    \"\"\"\n","    Generates a batch iterator for a dataset.\n","    \"\"\"\n","    data = np.array(data)\n","    data_size = len(data)\n","    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n","    for epoch in range(num_epochs):\n","        # Shuffle the data at each epoch\n","        if shuffle:\n","            shuffle_indices = np.random.permutation(np.arange(data_size))\n","            shuffled_data = data[shuffle_indices]\n","        else:\n","            shuffled_data = data\n","        for batch_num in range(num_batches_per_epoch):\n","            start_index = batch_num * batch_size\n","            end_index = min((batch_num + 1) * batch_size, data_size)\n","            yield shuffled_data[start_index:end_index]\n","\n"],"execution_count":0,"outputs":[]}]}