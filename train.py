# -*- coding: utf-8 -*-
"""Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WVyqSb7-I-V72h5OdV3YxtFvs1YsVzsL
"""

import numpy as np
from tensorflow.contrib import learn

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import LSTM
from keras.layers import Conv1D, MaxPooling1D

"""# download glove twitter data"""

from google_drive_downloader import GoogleDriveDownloader as gdd
gdd.download_file_from_google_drive(file_id='1EYPs1jjOYqe46gBDAreFCtXjGhKqxZKI', dest_path='./glove.zip', unzip=True)

!pip install import-ipynb
import import_ipynb
# Install the PyDrive wrapper & import libraries.
# This only needs to be done once per notebook.
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
# This only needs to be done once per notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Copy the link and remove the front part of the link (i.e. https://drive.google.com/open?id=) to get the file ID.
your_module = drive.CreateFile({'id':'1Sh3IHhb3HW-0nNPBkKln4GmRGPuEqR6s'})
your_module.GetContentFile('processingdata.ipynb')
import processingdata

"""# processing data to 'good' text"""

goodfile='good_Sentiment Analysis Dataset.csv'
badfile='bad_Sentiment Analysis Dataset.csv'
x_text, y = processingdata.get_dataset(goodfile, badfile, 1000000)

print(len(x_text))

"""# pre_trained data in glove twitter"""

your_module = drive.CreateFile({'id':'1OGJqKLf2kIbfWlQd-xzAxgVBk6PDD5Q2'})
your_module.GetContentFile('embedding_layer.ipynb')
import embedding_layer

max_document_length = max([len(x.split(" ")) for x in x_text])
print(max_document_length)

word_to_index, index_to_word, word_to_vec_map = embedding_layer.read_glove_vecs('glove.twitter.27B.100d.txt')



X_indices1=embedding_layer.sentense_to_indices(x_text,max_document_length,word_to_index)



print(X_indices1.shape)

max_document_length=X_indices1.shape[1]
print(max_document_length)

"""# spilt dataset"""

from sklearn.model_selection import train_test_split
train_x, test_validation_x, train_y, test_validation_y  = train_test_split(X_indices1, y, test_size=0.2, random_state=2019)
val_x, test_x, val_y, test_y = train_test_split(test_validation_x, test_validation_y, test_size=0.5, random_state=2018)

"""# Build model"""

import numpy as np
np.random.seed(0)
from keras.models import Model
from keras.layers import Dense, Input, Dropout, LSTM, Activation
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.initializers import glorot_uniform
np.random.seed(1)
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import GlobalAveragePooling1D,Bidirectional,GlobalMaxPooling1D
from keras.layers.convolutional import Convolution1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers.embeddings import Embedding
from keras.layers import LSTM,CuDNNLSTM
import keras
from keras.layers import Dense, Dropout, Activation
from keras.layers import Conv1D, MaxPooling1D

def pretrained_embedding_layer(word_to_vec_map, word_to_index):
  vocab_len=len(word_to_index)+1 # adding 1 to fit Keras embedding (requirement)
  embed_dim=100
   # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)
  emb_matrix = np.zeros((vocab_len, embed_dim))
    
    # Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary
  for word, index in word_to_index.items():
        emb_matrix[index, :] = word_to_vec_map[word]

    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. 
  embedding_layer = Embedding(vocab_len, embed_dim, trainable=False)
    ### END CODE HERE ###

    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".
  embedding_layer.build((None,))
    
    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.
  embedding_layer.set_weights([emb_matrix])
    
  return embedding_layer

def sentiment_analysis(input_shape, word_to_vec_map, word_to_index):
  sentence_indices = Input(input_shape, dtype='int32')
   # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)
  embed = pretrained_embedding_layer(word_to_vec_map, word_to_index)
    
    # Propagate sentence_indices through your embedding layer, you get back the embeddings
  embeddings = embed(sentence_indices) 

  x=CuDNNLSTM(units=128, return_sequences=True)(embeddings)
  x=Dropout(0.25)(x)
  x=CuDNNLSTM(units=128, return_sequences=True)(x)

  
  x=Conv1D(64,
                 3,
                 padding='valid',
                 activation='relu',
                 strides=1)(x)
# we use max pooling:
  x=GlobalMaxPooling1D()(x)

  x=Dense(32)(x)
  x=Activation('relu')(x)

  x=Dense(1)(x)
  x=Activation('sigmoid')(x)
  model = Model(inputs=sentence_indices, outputs=x)
    

  return model

model= sentiment_analysis((max_document_length,), word_to_vec_map, word_to_index)

model.summary()

mcp = keras.callbacks.ModelCheckpoint("my_model2.h5", monitor="val_acc",verbose=2,mode='max',
                      save_best_only=True)
    
    # Compile model
model.compile(optimizer='adam', loss='binary_crossentropy',
                  metrics=['accuracy'])
    
batch_size=128
epochs=20

history=model.fit(train_x, train_y,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=(val_x, val_y),callbacks=[mcp])



import matplotlib.pyplot as plt

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.savefig('books_read1.png')
plt.show()


# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.savefig('books_read2.png')
plt.show()

"""# evaluate the test dataset"""



from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/My Drive/aominebka991/sentiment analysis twitter project'

from keras.models import load_model
saved_model = load_model('my_model2.h5')

from keras.models import load_model
saved_model = load_model('my_model2.h5')
# evaluate the model

scores = saved_model.evaluate(test_x, test_y, verbose=1,batch_size=256)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])

def predict_sample(model,lstr,max_document_length,word_to_index,threshold=0.5):
  
  lstr=[processingdata.clean_str(y) for y in lstr]
  lstr_indices=embedding_layer.sentense_to_indices(lstr,max_document_length,word_to_index)
  face=model.predict(lstr_indices)

  sentiment=[]
  for x in face:
    if x >= threshold: sentiment.append(1)
    else: sentiment.append(0)
  return sentiment

predict_sample(saved_model,["Machine learning is so hard to learn but i still like it"],max_document_length,word_to_index,threshold=0.5)































